# linear regression
import tensorflow as tf

# 1. Build graph using TF operations ==> 트레이닝을 위한 그래프 생성(구현)
# X and Y data
X = tf.placeholder(tf.float32, shape=None)
Y = tf.placeholder(tf.float32, shape=None)

# W, b tensor(node) 정의
W = tf.Variable(tf.random_normal([1]), name='weight')
b = tf.Variable(tf.random_normal([1]), name='bias')

# Hypothesis H(x) = Wx + b
hypothesis = W * X + b

# cost(loss) function ==> tensorflow에서 표현하는 cost function
cost = tf.reduce_mean(tf.square(hypothesis - Y))

# Minimize using GradientDescentOptimizer function
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
train = optimizer.minimize(cost)

# 2. Run/update graph and get results ==> 그래프 구현 후, 세션을 만든다.
# Create Session
sess = tf.Session()
# Initializes global variables in the graph ==> variable을 이용하기 전에 반드시 초기화 해야 함!
sess.run(tf.global_variables_initializer())

# Fit the line
for step in range(2001):
    cost_val, W_val, b_val, _ = sess.run([cost, W, b, train], feed_dict={X: [1, 2, 3, 4, 5], Y: [2.1, 3.1, 4.1, 5.1, 6.1]}) # feed_dict를 이용하여 데이터 전달! python dict
    if step % 20 == 0:
        print(step, cost_val, W_val, b_val)


# 실행결과
# 0 12.955893 [0.54571855] [-0.22617087]
# 20 0.19915576 [1.2851163] [0.05724182]
# 40 0.17369755 [1.269649] [0.12642066]
# 60 0.15169127 [1.252004] [0.19018517]
# 80 0.13247314 [1.2355001] [0.24976967]
# 100 0.11568977 [1.2200772] [0.3054519]
# 120 0.10103278 [1.2056639] [0.3574875]
# 140 0.088232614 [1.1921949] [0.40611535]
# 160 0.07705425 [1.179608] [0.45155832]
# 180 0.06729202 [1.1678452] [0.49402535]
# 200 0.058766603 [1.1568528] [0.5337111]
# 220 0.051321357 [1.1465806] [0.57079774]
# 240 0.044819303 [1.1369809] [0.6054556]
# 260 0.039141025 [1.1280098] [0.6378438]
# 280 0.034182157 [1.1196264] [0.6681107]
# 300 0.02985153 [1.111792] [0.69639546]
# 320 0.026069555 [1.1044706] [0.72282785]
# 340 0.022766758 [1.0976287] [0.74752915]
# 360 0.019882377 [1.0912349] [0.7706127]
# 380 0.017363446 [1.0852599] [0.79218453]
# 400 0.0151636 [1.0796763] [0.81234354]
# 420 0.013242489 [1.0744581] [0.8311825]
# 440 0.01156476 [1.0695817] [0.8487875]
# 460 0.0100996 [1.0650249] [0.8652397]
# 480 0.008820042 [1.0607662] [0.8806143]
# 500 0.0077026086 [1.0567867] [0.8949821]
# 520 0.006726729 [1.0530677] [0.90840894]
# 540 0.0058745258 [1.0495921] [0.9209563]
# 560 0.0051302696 [1.0463444] [0.93268204]
# 580 0.004480296 [1.0433092] [0.9436398]
# 600 0.0039126836 [1.0404729] [0.9538799]
# 620 0.003416966 [1.0378224] [0.96344936]
# 640 0.0029840614 [1.0353453] [0.9723922]
# 660 0.0026060084 [1.0330305] [0.9807493]
# 680 0.002275852 [1.0308673] [0.98855907]
# 700 0.0019875132 [1.0288458] [0.99585736]
# 720 0.0017357182 [1.0269567] [1.0026777]
# 740 0.0015158205 [1.0251913] [1.0090512]
# 760 0.0013237781 [1.0235416] [1.0150075]
# 780 0.0011560686 [1.0219998] [1.0205736]
# 800 0.0010096036 [1.0205592] [1.0257752]
# 820 0.0008816874 [1.0192126] [1.0306363]
# 840 0.0007699875 [1.0179542] [1.0351791]
# 860 0.0006724357 [1.0167786] [1.0394241]
# 880 0.0005872522 [1.0156797] [1.043391]
# 900 0.00051284686 [1.0146528] [1.0470985]
# 920 0.00044787518 [1.0136932] [1.0505629]
# 940 0.0003911321 [1.0127965] [1.0538006]
# 960 0.00034158063 [1.0119585] [1.0568262]
# 980 0.0002983042 [1.0111754] [1.0596539]
# 1000 0.00026051205 [1.0104434] [1.0622962]
# 1020 0.00022750642 [1.0097593] [1.0647653]
# 1040 0.00019868312 [1.0091202] [1.067073]
# 1060 0.00017350298 [1.0085229] [1.0692298]
# 1080 0.0001515229 [1.0079646] [1.071245]
# 1100 0.00013232489 [1.007443] [1.0731282]
# 1120 0.000115562485 [1.0069556] [1.074888]
# 1140 0.00010092164 [1.0065] [1.0765326]
# 1160 8.8135814e-05 [1.0060743] [1.0780694]
# 1180 7.697161e-05 [1.0056766] [1.0795054]
# 1200 6.722084e-05 [1.0053048] [1.0808476]
# 1220 5.870281e-05 [1.0049574] [1.082102]
# 1240 5.1265884e-05 [1.0046327] [1.0832741]
# 1260 4.477046e-05 [1.0043294] [1.0843694]
# 1280 3.9099898e-05 [1.004046] [1.085393]
# 1300 3.414705e-05 [1.003781] [1.0863494]
# 1320 2.9820489e-05 [1.0035334] [1.0872433]
# 1340 2.6042355e-05 [1.0033019] [1.0880789]
# 1360 2.2743394e-05 [1.0030857] [1.0888596]
# 1380 1.9861876e-05 [1.0028837] [1.089589]
# 1400 1.7346209e-05 [1.0026948] [1.0902708]
# 1420 1.5147709e-05 [1.0025183] [1.090908]
# 1440 1.3228526e-05 [1.0023534] [1.0915035]
# 1460 1.1552761e-05 [1.0021992] [1.09206]
# 1480 1.0089377e-05 [1.0020553] [1.0925798]
# 1500 8.8118195e-06 [1.0019206] [1.0930657]
# 1520 7.694754e-06 [1.0017949] [1.0935199]
# 1540 6.7201013e-06 [1.0016773] [1.0939442]
# 1560 5.868871e-06 [1.0015675] [1.0943408]
# 1580 5.1252346e-06 [1.0014648] [1.0947114]
# 1600 4.4762874e-06 [1.001369] [1.0950578]
# 1620 3.908759e-06 [1.0012794] [1.0953814]
# 1640 3.4144105e-06 [1.0011955] [1.0956837]
# 1660 2.9813255e-06 [1.0011172] [1.0959665]
# 1680 2.6035164e-06 [1.001044] [1.0962306]
# 1700 2.27372e-06 [1.0009757] [1.0964775]
# 1720 1.9856589e-06 [1.0009118] [1.0967084]
# 1740 1.734073e-06 [1.0008521] [1.096924]
# 1760 1.5144853e-06 [1.0007962] [1.0971253]
# 1780 1.3225729e-06 [1.0007441] [1.0973136]
# 1800 1.1546978e-06 [1.0006953] [1.0974895]
# 1820 1.0084743e-06 [1.0006498] [1.0976539]
# 1840 8.809564e-07 [1.0006074] [1.0978074]
# 1860 7.692669e-07 [1.0005677] [1.0979508]
# 1880 6.7203285e-07 [1.0005306] [1.0980849]
# 1900 5.8706854e-07 [1.0004958] [1.0982101]
# 1920 5.1263765e-07 [1.0004634] [1.0983274]
# 1940 4.4769376e-07 [1.000433] [1.0984368]
# 1960 3.9107482e-07 [1.0004046] [1.0985391]
# 1980 3.4161917e-07 [1.0003783] [1.0986346]
# 2000 2.9837815e-07 [1.0003536] [1.0987238]